#+TITLE: Operate Resources in Public and Private Clouds
#+TITLE: Thanks to OpenStack
#+AUTHOR: Ronan-Alexandre Cherrueau, Adrien Lebre, Didier Iscovery
#+EMAIL: {firstname.lastname}@inria.fr
#+DATE: [2019-09-30 Mon]

#+STARTUP: entitiespretty
#+LANGUAGE: en
#+OPTIONS: ^:{} ':t email:t toc:nil
#+PROPERTY: header-args :mkdirp yes
#+MACRO: co  OPH
#+MACRO: c5o Online Polytech Hosting
#+LINK: base-url https://rcherrueau.github.io/teaching/2019/os-polytech-laroche/%s
#+LINK: cdn-url  https://raw.githubusercontent.com/BeyondTheClouds/lectures/master/%s
#+LINK: horizon-url  http://localhost/%s

# -- HTML specific options
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto html-preamble:t html-scripts:t html-style:t html5-fancy:t tex:t
#+HTML_DOCTYPE: html5
#+HTML_CONTAINER: div
# #+HTML_LINK_HOME: ../index.html
# #+HTML_LINK_UP: ../index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../../../rsc/org.css" />
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../../rsc/org.css" />
#+HTML_HEAD: <style>#table-of-contents .tag {display: none;}</style>
#+HTML_HEAD_EXTRA:
#+CREATOR: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 26.1 (<a href="https://orgmode.org">Org</a> mode 9.2) - theme by <a href=\"http://gongzhitaao.org/orgcss\">http://gongzhitaao.org/orgcss</a>

#+EXCLUDE_TAGS: noexport
#+EXCLUDE_TAGS: solution

#+BEGIN_abstract
OpenStack has become the de-facto solution to operate compute, network
and storage resources in public and private clouds. In this lab, we
are going to:
- Deploy an all-in-one OpenStack with [[https://opendev.org/x/microstack/][Snap microstack]].
- Operate this OpenStack to manage IaaS resources (e.g., boot VMs,
  setup a private Network).
- Deploys a Wordpress as a Service.

Find the slides of the lecture [[cdn-url:2018/os-polytech/docs/CloudFogEdgeIntro.pdf][here]] and [[cdn-url:2018/os-polytech/docs/openstack-slides.pdf][there]].
#+END_abstract
# This document is an [[https://orgmode.org/][Org mode]] document, you can find its source [[base-url:index.org][here]].

#+TOC: headlines 2

* Table of Contents                                       :TOC_3_gh:noexport:
- [[#requirements-and-setup][Requirements and Setup]]
  - [[#environment][Environment]]
  - [[#resources-of-the-lab][Resources of the Lab]]
  - [[#setup-openstack][Setup OpenStack]]
- [[#play-with-openstack-as-an-admin][Play with OpenStack (as an Admin)]]
  - [[#openstack-horizon-dashboard][OpenStack Horizon Dashboard]]
  - [[#unleash-the-operator-in-you][Unleash the Operator in You]]
  - [[#in-encryption-we-trust][In Encryption We Trust]]
  - [[#the-art-of-contextualizing-a-vm][The Art of Contextualizing a VM]]
    - [[#debian-9-ftw][Debian 9 FTW]]
    - [[#cloud-init-in-action][~cloud-init~ in Action]]
  - [[#run-vms-with-kvm][Run VMs with KVM]]
- [[#deploy-a-wordpress-as-a-service-as-a-devops][Deploy a WordPress as a Service (as a DevOps)]]
- [[#appendix][Appendix]]
  - [[#install-mariadb-on-debian-9][Install MariaDB on Debian 9]]
  - [[#install-wordpress-application-on-debian-9][Install Wordpress application on Debian 9]]

* Lecture Notes                                                    :noexport:
** Problem with Virtualbox
See https://github.com/CanonicalLtd/microstack/issues/41

Change ~/var/snap/microstack/common/etc/nova/nova.conf.d/hypervisor.conf~

#+BEGIN_SRC conf
[libvirt]
virt_type = qemu
cpu_mode = host-model
#+END_SRC

And restart ~nova-compute~

: sudo systemctl restart snap.microstack.nova-compute.service

** Export and publish
#+BEGIN_SRC elisp :results silent :noweb yes
<<export>>
<<publish>>
#+END_SRC

*** Export
Do ~C-c C-c~ on the following
#+NAME: export
#+BEGIN_SRC elisp :results silent :noweb yes
(delete-directory "rsc" t)
(org-babel-tangle)
(org-ascii-export-to-ascii)
(org-html-export-to-html)

;; Make the tp.tar.gz
(defun f-cmd (&rest cmds) (s-join " " cmds))
(shell-command (f-cmd "tar czf tp.tar.gz"
                      "index.txt" "setup.sh"
                      "teardown.sh" "rsc"))
#+END_SRC

*** Publish
Put it on my personal website and change the link to org file to link
the one in
#+NAME: publish
#+BEGIN_SRC elisp :results silent :noweb yes
(let* ((base-url "https://rcherrueau.github.io")
       (base-dir "~/prog/rcherrueau.github.com/teaching/")
       (export-dir (concat base-dir "2019/os-polytech-laroche/")))
  ;; Delete export if it exists. Always start from the ground base.
  (when (file-directory-p export-dir)
    (delete-directory export-dir t))

  ;; Create os-imt directory and copy index files
  (make-directory export-dir)
  ;; (shell-command (format "cp -r index.org %s" export-dir))
  (shell-command (format "cp -r index.html %s" export-dir))

  ;; Link with online css
  ;; (find-file (concat export-dir "index.html"))
  ;; (with-current-buffer "index.html"
  ;;   (goto-char (point-min))
  ;;   (while (re-search-forward "href=\"org.css\"" nil t)
  ;;     (replace-match (format "href=\"%s/rsc/org.css\"" base-url)))
  ;;   (save-buffer))
)
#+END_SRC

*** Hide/show solutions
Hide solutions
#+BEGIN_SRC elisp :results silent :noweb yes
(save-excursion
  (while (re-search-forward "\\(BEGIN\\|END\\)_solution" nil t)
    (replace-match "\\1_comment\n# solution"))
  (save-buffer))
#+END_SRC

Show solutions
#+BEGIN_SRC elisp :results silent :noweb yes
(save-excursion
  (while (re-search-forward "\\(BEGIN\\|END\\)_comment\n# solution" nil t)
    (replace-match "\\1_solution"))
  (save-buffer))
#+END_SRC

* Requirements and Setup
:PROPERTIES:
:CUSTOM_ID: sec:req-setup
:END:
** Environment
To follow the lab you'll need a Linux (prefer Ubuntu 18.04) with 8 Go
of RAM and a few Go of disk.

The lab makes use of [[https://opendev.org/x/microstack/][Snap microstack]]: OpenStack in a Snap that you can
run locally on a single machine. Snap is the Canonical's App delivery
mechanism. It enables developers to bundle all dependencies into a
single app package. And so does Snap microstack for an all-in-one
OpenStack on your machine.

An all-in-one OpenStack means that your machine will contain both
services to /operate/ and /host/ virtualized resources. For instance,
the ~nova-conductor~ to operate the boot of a VM, and ~nova-compute~
to host the VM. This is a good setup for a lab, but not for
production. There are several other options such as [[https://docs.openstack.org/devstack/latest/index.html][DevStack]],
[[https://docs.openstack.org/puppet-openstack-guide/latest/][Puppet-OpenStack]] or [[https://docs.openstack.org/developer/kolla-ansible/][Kolla-ansible]] and all matters. But, Snap
microstack takes only 2 minutes to deploy OpenStack (instead of 30
minutes for other options).

#+BEGIN_note
- Devstack is good for OpenStack developers.
- Puppet-OpenStack or Kolla-ansible are good for production
  deployments.
#+END_note

** Resources of the Lab
:PROPERTIES:
:CUSTOM_ID: sec:rscs-lab
:END:
Get the resources of the lab at [[cdn-url:2019/os-polytech-laroche/tp.tar.gz]].

#+BEGIN_SRC bash
curl -O https://raw.githubusercontent.com/BeyondTheClouds/lectures/master/2019/os-polytech-laroche/tp.tar.gz
mkdir ~/os-oph
tar xzf tp.tar.gz -C ~/os-oph
cd ~/os-oph
#+END_SRC

The archive contains:
- index.txt :: The current subject in text format.
- setup.sh :: Script that sets up the lab.
- teardown.sh :: Script that uninstalls the lab.
- rsc :: Resource directory with bash scripts useful for the lab.

** Setup OpenStack
Install snap.
: sudo apt update
: sudo apt install snapd

Install OpenStack directly from the snap store.
: sudo snap install microstack --classic --candidate

Then, ensure OpenStack services are running on your machine.

#+BEGIN_do
Find the snap command that lists microstack OpenStack services and
there status? What is the purpose of each service?

#+BEGIN_comment
# solution
: snap services microstack

- glance-* :: Glance to manage VM images: ~openstack image --help~.
- horizon-* :: OpenStack Web dashboard: [[http://<ip-of-your-Lab-machine>]].
- keystone-* :: Keystone to manage authentication and authorization
                on OpenStack.
- neutron-* :: Neutron to manage networks: ~openstack network --help~.
- nova-* :: Nova to manage VM: ~openstack server --help~.
- memcached :: Cache used by all OpenStack services
- mysqld :: Database used by all OpenStack services
- rabbitmq-server :: Communication bus used by all OpenStack services
#+END_comment
# solution
#+END_do

And finally, execute the ~setup.sh~ file with sudo.
: sudo ./setup.sh

#+BEGIN_SRC bash :tangle ./setup.sh :shebang #!/usr/bin/env bash :exports none
# Install the bare necessities
set -o errexit
set -o xtrace

apt install -y curl tcpdump kmod
snap install openstackclients --classic --candidate

# Make nova use qemu instead of qemu-kvm
# i.e,:
# > [libvirt]
# > virt_type = kvm             # rewrite to qemu
# > cpu_mode = host-passthrough # rewrite to host-model
NOVA_HYPERV_CONF=/var/snap/microstack/common/etc/nova/nova.conf.d/hypervisor.conf
sed -i 's|virt_type.\+|virt_type = qemu|' $NOVA_HYPERV_CONF
sed -i 's|cpu_mode.\+|cpu_mode = host-model|' $NOVA_HYPERV_CONF
snap restart microstack.nova-compute

# Workaround to modify openstack configurartion files (the `--classic`
# mode doesn't help here): Setup an overlay and change horizon conf to
# make it listen on any host
OV_NAME="snap-microstack-overlay"
if ! fgrep -q "$OV_NAME on" <<< $(mount -l)
then
    HORIZON=/snap/microstack/current/lib/python2.7/site-packages/openstack_dashboard/local
    UPP_BIN="$(mktemp -d)"
    WORK_BIN="$(mktemp -d)"

    mount --types overlay --options \
        lowerdir=$HORIZON,upperdir=$UPP_BIN,workdir=$WORK_BIN \
        $OV_NAME $HORIZON

    echo "ALLOWED_HOSTS = ['*']" >> $HORIZON/local_settings.py
    snap restart microstack.horizon-uwsgi
fi

# Put snap openstackclients into the path.
export PATH=/snap/bin:$PATH
#+END_SRC

#+BEGIN_SRC bash :noweb tangle :tangle ./teardown.sh :shebang #!/usr/bin/env bash :exports none
set -o errexit
set -o xtrace

. admin-openrc.sh

<<lst:delete-rscs>>

sudo snap remove openstackclients
sudo snap remove microstack
sudo sysctl -w net.ipv4.ip_forward=0
read -p 'Ip of your host machine (to remove iptables SNAT): '  IP_LAB
sudo iptables -t nat -A POSTROUTING ! -d 10.20.20.0/24 -o eth0 -j SNAT --to-source ${IP_LAB}
#+END_SRC

* Play with OpenStack (as an Admin)
:PROPERTIES:
:CUSTOM_ID: sec:play-with-os
:END:
** OpenStack Horizon Dashboard
One service deployed is the OpenStack dashboard (Horizon). On your own
machine, horizon is reachable from the web browser at
[[http://<ip-of-your-Lab-machine>]] with the following credentials:
- login: ~admin~
- password: ~keystone~

From here, you can reach ~Project > Compute > Instances > Launch
Instance~ and boot a virtual machine given the following information:
- a name (e.g., ~horizon-vm~)
- an image (e.g., ~cirros~)
- a flavor to limit the resources of your instance (we recommend
  ~m1.tiny~)
- and a network setting (must be ~test~)

You should select options by clicking on the arrow on the right of
each possibility. When the configuration is OK, the ~Launch Instance~
button should be enabled. After clicking on it, you should see the
instance in the ~Active~ state in less than a minute.

Now, you have several options to connect to your freshly deployed VM.
For instance, after clicking on its name, Horizon provides a virtual
console under the ~Console~ tab. So, you can use the following
credentials to access the VM:
- login: ~cirros~
- password: ~gocubsgo~

Unfortunately this feature is disabled with Snap microstack. But as a
real DevOps, you will prefer to access to your VM by the command line
interface ...

** Unleash the Operator in You
:PROPERTIES:
:CUSTOM_ID: sec:os-cli
:END:
While Horizon is helpful to discover OpenStack features, this is not
the tool of choice for a real operator. A real operator prefers
command line interface 😄. You are lucky, OpenStack provides such a
command line interface.

To use it, you need to set your environment with the OpenStack
credentials, so that the command line won't bother you by requiring
credentials each time. You can retrieve this information through the
Horizon interface by clicking on the ~admin~ dropdown list at the top
right corner and get the "OpenStack RC File V3" (or by following
[[http://<ip-of-your-Lab-machine>/project/api_access/openrc/]]).

To setup your environment please download and source this file on your
Lab machine.
: source ./admin-openrc.sh

You can then check that your environment is correctly set.
#+begin_src bash
$ env|fgrep OS_|sort

OS_AUTH_URL==http://10.20.20.1:5000/v3/
OS_IDENTITY_API_VERSION=3
OS_INTERFACE=public
OS_PASSWORD=keystone
OS_PROJECT_DOMAIN_ID=default
OS_PROJECT_ID=76c02713292e4d3cba0625c9995a96aa
OS_PROJECT_NAME=admin
OS_REGION_NAME=microstack
OS_USER_DOMAIN_NAME=Default
OS_USERNAME=admin
#+end_src

All operations to manage OpenStack are done through one unique command
line, called ~openstack <service> <action> ...~. Doing an ~openstack
--help~ displays the /really long/ list of services/possibilities
provided by this command. The following gives you a selection of the
most often used commands to operate your Cloud:
- List OpenStack running services :: ~openstack endpoint list~
- List images :: ~openstack image list~
- List flavors :: ~openstack flavor list~
- List networks :: ~openstack network list~
- List computes :: ~openstack hypervisor list~
- List VMs (running or not) :: ~openstack server list~
- Get details on a specific VM :: ~openstack server show <vm-name>~
- Start a new VM :: ~openstack server create --image <image-name> --flavor <flavor-name> --nic net-id=<net-id> <vm-name>~
- View VMs logs :: ~openstack console log show <vm-name>~

#+BEGIN_do
Using all these commands, you can use the CLI to start a new tiny
cirros VM called ~cli-vm~.
#+BEGIN_comment
# solution
#+BEGIN_src bash
openstack server create \
  --image cirros \
  --flavor m1.tiny \
  --network test \
  cli-vm
#+END_src
#+END_comment
# solution
#+END_do

Then, display the information about your VM with the following
command:
: openstack server show cli-vm

Note in particular the ~status~ of your VM.
: openstack server show cli-vm -c status -f json

This status will go from ~BUILD~: OpenStack is looking for the best
place to boot the VM; to ~ACTIVE~: your VM is running. The status
could also be ~ERROR~ if you are experiencing hard times with your
infrastructure.

A VM in ~ACTIVE~ state still has to go through the [[http://www.tldp.org/LDP/intro-linux/html/sect_04_02.html][boot process and
init]]. Hence, you may wait for one minute or two, the time for the VM
finishing booting. You can check that by looking at its logs with
~openstack console log show cli-vm~. The VM finished to boot when last
lines are:
#+BEGIN_EXAMPLE
=== cirros: current=0.4.0 latest=0.4.0 uptime=29.16 ===
  ____               ____  ____
 / __/ __ ____ ____ / __ \/ __/
/ /__ / // __// __// /_/ /\ \
\___//_//_/  /_/   \____/___/
   http://cirros-cloud.net


login as 'cirros' user. default password: 'gocubsgo'. use 'sudo' for root.
cli-vm login:
#+END_EXAMPLE

With the previous ~openstack server create~ command, the VM boots with
a private IP. Private IPs are used for communication between VMs,
meaning you cannot ping your VM from an external network (e.g., the
Lab machine). You have to manually affect a floating IP of the
~external~ network to your machine if you want it to be pingable from
the Lab.
#+BEGIN_SRC bash
ALLOCATED_FIP=$(openstack floating ip create \
  -c floating_ip_address -f value external)
openstack server add floating ip cli-vm "$ALLOCATED_FIP"
#+END_SRC

Then, ask again for the status of your VM and its IPs.
: openstack server show cli-vm -c status -c addresses

#+BEGIN_do
Ping ~cli-vm~ on its floating IP.
: ping "$ALLOCATED_FIP"

Does it work? Why? Hint: [[https://docs.openstack.org/neutron/latest/feature_classification/general_feature_support_matrix.html#operation_Security_Groups][OpenStack sets security groups by default]].
Find the command that list the security group rules of the ~admin~
project. # (i.e., ~openstack project show admin~).

#+BEGIN_comment
# solution
Regarding security rules, OpenStack is very conservative by default
and prevents ingress and egress traffic. Spot the ~None~ values at ~IP
Protocol~ and ~IP Range~ in the result table of the command that list
security group rules of the admin project: These values should be
interpreted as /"~None~ protocol in ~None~ network is allowed"/.
#+BEGIN_src bash
$ SECGROUP_ID=`openstack security group list --project admin -f value -c ID`
$ openstack security group rule list -c ID -c "IP Protocol" -c "IP Range" $SECGROUP_ID

+--------------------------------------+-------------+-----------+
| ID                                   | IP Protocol | IP Range  |
+--------------------------------------+-------------+-----------+
| 1b892ef7-35a5-43ca-93e8-8748945f0ee8 | None        | None      |
| 1ea8cc81-8aa6-4e11-a759-606ba2e16247 | None        | None      |
| 7439b698-6e10-45aa-877f-075e49769dd0 | None        | None      |
| 9177e69a-ab91-4016-b8a7-b98e04588eee | None        | None      |
+--------------------------------------+-------------+-----------+
#+END_src
#+END_comment
# solution

Then, make it work. See examples of security groups rules in the [[https://docs.openstack.org/neutron/latest/admin/deploy-lb-selfservice.html#verify-network-operation][neutron
doc]].

#+BEGIN_comment
# solution
To make it work, you have to setup new rules in the security group of
the ~admin~ project. The following rules allow ICMP packets (for ping)
and TCP on port 22 (for SSH connection) on the VM.
#+BEGIN_src bash
openstack security group rule create $SECGROUP_ID --proto icmp --remote-ip 0.0.0.0/0
openstack security group rule create $SECGROUP_ID --proto tcp --remote-ip 0.0.0.0/0 \
  --dst-port 22
#+END_src
#+END_comment
# solution
#+END_do

Once you succeed to ping the vm, you should be able to SSH on it
: ssh -l cirros "$ALLOCATED_FIP"

#+BEGIN_comment
*Note for the teacher:* The [[lst:undo-net-setup]] code undoes the
[[https://opendev.org/x/microstack/src/commit/c5f679a67377e0e963e79603ef2cfb23a706df28/snap-overlay/bin/setup-br-ex#L21][microstack network setup]], so students have to resolve the next
challenge (i.e., the next ~begin_do~). This bash snippet is tangle
into file:setup.sh and could be disable in case of no network-oriented
students.
#+END_comment
#+name: lst:undo-net-setup
#+begin_src bash :tangle ./setup.sh :exports none
set +o xtrace
sysctl -w net.ipv4.ip_forward=0 > /dev/null
extcidr=10.20.20.0/24  # find it with `sudo iptables -t nat -L`
iptables -w -t nat -D POSTROUTING -s $extcidr ! -d $extcidr -j MASQUERADE > /dev/null
#+end_src

#+BEGIN_do
From the cirros, ping the outside world.
: ping 8.8.8.8  # GOOGLE could you HEAR me?!

Does it work? Why? To help you in your diagnosis, here is a list of
hints to check:
- Ping Google and the VM from the Lab machine. Does it work?
  #+BEGIN_comment
# solution
  : ping -c 2 8.8.8.8; ping -c 2 $ALLOCATED_FIP
  The ping from the Lab machine works for both Google and the VM.
  Thus, the Lab machine /could be a gateway/ between VMs and the
  Internet.
  #+END_comment
# solution

- Note the IP address of ~$ALLOCATED_FIP~. From which network this IP
  comes? Which NIC serves that network on the Lab machine?
  #+BEGIN_comment
# solution
  : echo "$ALLOCATED_FIP"
  : openstack subnet show external-subnet -c cidr -c allocation_pools
  : ip address | fgrep -B 2 10.20.20
  The IP of the VM comes from the network 10.20.20.0/24, which is
  served on the Lab machine by ~br-ex~.
  #+END_comment
# solution

- Do a ~tcpdump~ on that NIC. Do you see the ICMP packets from
  ~$ALLOCATED_FIP~ that flow over that NIC?
  #+BEGIN_comment
# solution
  : sudo tcpdump -nni br-ex icmp
  The ~tcpdump~ on ~br-ex~ shows ping ~echo request~ packets, but no
  ~echo reply~. So the packets are lost somewhere.... In other words,
  the Lab machine does not play its role of gateway between VMs and
  the Internet.
  #+END_comment
# solution

- Find the route that forward packets to the Internet on Lab machine.
  Do a ~tcpdump~ on the NIC that servers that route. Do you see the
  ICMP packets flow over that NIC?
  #+BEGIN_comment
# solution
  To ensure the Lab machine does not play its role of gateway between
  VMs and the Internet, let's find the route that forward Google
  packets out of the Lab machine.
  : $ ip route
  :
  : default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.77 metric 100
  : 10.20.20.0/24 dev br-ex proto kernel scope link src 10.20.20.1
  : 192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.77
  : 192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.77 metric 100
  The command does not show up an /explicit/ route for ~8.8.8.0/9~
  packets. This means that packets are supposed to flow through the
  /default/ route served by the ~eth0~ NIC on my machine.

  Next, do a ~tcpdump~ on that NIC to see if the ICMP packet go
  through it.
  : sudo tcpdump -nni eth0 icmp
  Nothing appears. So ICMP packet are lost somewhere between ~br-ex~
  and ~eth0~, despite the first hint.

  To put it differently, the Lab machine does not forward the incoming
  traffic on ~br-ex~ to ~eth0~. And this is normal, there is [[https://serverfault.com/questions/749682/ip-forwarding-on-linux-anything-important-to-make-sure-to-do-or-know][no reason]]
  for Linux to enable this by default. However in our case, we have to
  activate it. This is called /Kernel IP Forwarding/, and it could be
  set up with the next command (or ~echo 1 >
  /proc/sys/net/ipv4/ip_forward~).
  : sudo sysctl -w net.ipv4.ip_forward=1
  #+END_comment
# solution

- After making the packets flow on the second NIC, is everything OK
  with the IP address of the source in the ~tcpdump~ on ~eth0~?
  #+BEGIN_comment
# solution
  From now, the ping of Google from the VM reaches Internet via ~eth0~
  (as seen by ~tcpdump -nni eth0 icmp~). Unfortunately, it still
  doesn't do the trick, because the packet goes out with the
  ~10.20.20.*~ source address. For this reason, Google sees ~ICMP echo
  request~ incoming packets from ~10.20.20.*~ and hence, replies ~ICMP
  echo reply~ to ~10.20.20.*~ which does not makes sense except on the
  Lab machine.

  You have to change the source IP of out packet (~10.20.20.*~) to
  gateway's IP (i.e., Your lab machine). The ~iptables~ will then
  automatically change the replied packet's destination IP
  (~<ip-of-your-Lab-machine>~) to the original source IP
  (~10.20.20.*~). This process is called a SNAT and you can implement
  it with ~iptables~ (see,
  https://www.systutorials.com/1372/setting-up-gateway-using-iptables-and-route-on-linux/).

  Set up the SNAT with ~iptables~.
  : sudo iptables -t nat -A POSTROUTING ! -d 10.20.20.0/24 -o eth0 -j SNAT --to-source <ip-of-your-Lab-machine>
  #+END_comment
# solution
#+END_do

Go on, and play with the ~openstack~ cli. For instance, list all
features offered by Nova with ~openstack server --help~ and try to
figure out how to:
1. SSH on ~cli-vm~ using its name rather than its IP;
2. Suspend and resume it;
3. Create a snapshot of ~cli-vm~;
4. Boot a new machine ~cli-vm-clone~ from the snapshot.
5. Delete ~cli-vm-clone~;

#+BEGIN_comment
# solution
#+BEGIN_SRC bash
# 1.
openstack server ssh cli-vm -l cirros
# 2.
openstack server suspend cli-vm; openstack server show cli-vm -c status
openstack server resume cli-vm; openstack server show cli-vm -c status
# 3.
openstack server image create --name cli-vm-img cli-vm; openstack image list
# 4.
openstack server create --wait --flavor m1.tiny \
  --network test --image cli-vm-img \
  cli-vm-clone
# 5.
openstack server delete cli-vm-clone
#+END_SRC
#+END_comment
# solution

** In Encryption We Trust
:PROPERTIES:
:CUSTOM_ID: sec:enc-trust
:END:
Any cirros VMs share the same credentials (i.e., ~cirros~, ~gocubsgo~)
which is a security problem. As an IaaS DevOps, you want that only
some clients can SSH on the VMs. Fortunately, OpenStack helps with the
management of SSH keys. OpenStack can generate a SSH key and push the
public counterpart on the VM. Therefore, doing a ~ssh~ on the VM will
use the SSH key instead of asking the client to fill the credentials.

Make an SSH key and store the private counterpart in =./admin.pem=.
Then, give that file the correct permission access.
: openstack keypair create --private-key ./admin.pem admin
: chmod 600 ./admin.pem

Start a new VM and ask OpenStack to copy the public counterpart of
your SSH key in the =~/.ssh/authorized_keys= of the VM (i.e., note the
~--key-name admin~).
#+BEGIN_SRC bash
openstack server create --wait --image cirros \
  --flavor m1.tiny --network test \
  --key-name admin cli-vm-adminkey
#+END_SRC

Attach it a floating IP.
#+BEGIN_SRC bash
openstack server add floating ip \
  cli-vm-adminkey \
  $(openstack floating ip create -c floating_ip_address -f value external)
#+END_SRC

Now you can access your VM using SSH without filling credentials.
#+BEGIN_SRC bash
openstack server ssh cli-vm-adminkey \
  --login cirros \
  --identity ./admin.pem
#+END_SRC

Or directly with the ~ssh~ command
: ssh -i ./admin.pem -l cirros $(openstack server show cli-vm-adminkey -c addresses -f value | sed  -Er 's/test=.+ (10\.20\.20\.[0-9]+).*/\1/g')

#+BEGIN_note
A regular ~ssh~ command looks like ~ssh -i <identity-file> -l <name>
<server-ip>~. The following OpenStack command followed by the ~sed~
returns the floating IP of ~cli-vm-adminkey~. You may have to adapt it
a bit according to your network cidr.
: openstack server show cli-vm-adminkey -c addresses -f value | sed  -Er 's/test=.+ (10\.20\.20\.[0-9]+).*/\1/g'
#+END_note

** The Art of Contextualizing a VM
Contextualizing is the process that automatically installs software,
alters configurations, and more on the machine as part of the boot
process. On OpenStack, contextualizing is achieved thanks to
[[https://cloud-init.io/][~cloud-init~]]. It is a program that runs at the boot time to customize
the VM.

You have already used ~cloud-init~ without even knowing it! The
previous command ~openstack server create~ with the ~--identity~
parameter tells OpenStack to make the public counterpart of the SSH
key available to the VM. When the VM boots for the first time,
~cloud-init~ is (among other tasks) in charge of fetching this public
SSH key from OpenStack, and copy it to =~/.ssh/authorized_keys=.
Beyond that, ~cloud-init~ is in charge of many aspects of the VM
customization like mounting volume, resizing file systems or setting
an hostname (the list of ~cloud-init~ modules can be found [[http://cloudinit.readthedocs.io/en/latest/topics/modules.html][here]]).
Furthermore, ~cloud-init~ is able to run a bash script that will be
executed on the VM as ~root~ during the boot process.

*** Debian 9 FTW
:PROPERTIES:
:CUSTOM_ID: sec:debian9-ftw
:END:
When it comes the time to deal with real applications, we cannot use
cirros VMs anymore. A Cirros VM is good for testing because it starts
fast and has a small memory footprint. However, do not expect to
launch [[https://en.wikipedia.org/wiki/MariaDB][MariaDB]] or even [[https://github.com/busyloop/lolcat][~lolcat~]] on a cirros.

We are going to run several Debian9 VMs in this section. But, a
Debian9 takes a lot more of resources to run. For this reason, you may
want to release all your resources before going further.

#+NAME: lst:delete-rscs
#+BEGIN_SRC bash
# Delete VMs
for vm in $(openstack server list -c ID -f value); do \
  echo "Deleting ${vm}..."; \
  openstack server delete "${vm}"; \
done

# Releasing floating IPs
for ip in $(openstack floating ip list -c "Floating IP Address" -f value); do \
  echo "Releasing ${ip}..."; \
  openstack floating ip delete "${ip}"; \
done
#+END_SRC

Then, download the Debian9 image with support of ~cloud-init~.
#+BEGIN_SRC bash
curl -L -o /tmp/debian-9.qcow2 \
  https://cdimage.debian.org/cdimage/openstack/current-9/debian-9-openstack-amd64.qcow2
#+END_SRC

#+BEGIN_do
Import the image into Glance; name it ~debian-9~. Use ~openstack image
create --help~ for creation arguments. Find values example with
~openstack image show cirros~.
#+BEGIN_comment
# solution
#+BEGIN_SRC bash
openstack image create --disk-format=qcow2 \
  --container-format=bare --property architecture=x86_64 \
  --public --file /tmp/debian-9.qcow2 \
  debian-9
#+END_SRC
#+END_comment
# solution

And, create a new ~m1.mini~ flavor with 5 Go of Disk, 2 Go of RAM, 2
VCPU and 1 Go of swap. Use ~openstack flavor create --help~ for
creation arguments.
#+BEGIN_comment
# solution
#+BEGIN_SRC bash
openstack flavor create --ram 2048 \
  --disk 5 --vcpus 2 --swap 1024 \
  --public m1.mini
#+END_SRC
#+END_comment
# solution
#+END_do

*** ~cloud-init~ in Action
:PROPERTIES:
:CUSTOM_ID: sec:cloud-init
:END:
To tell ~cloud-init~ to load and execute a specific script at boot
time, you should append the ~--user-data <file/path/of/your/script>~
extra argument to the regular ~openstack server create~ command.

#+BEGIN_do
Start a new VM named ~art-vm~ based on the ~debian-9~ image and the
~m1.mini~ flavor. The VM should load and execute the script [[lst:art.sh]]
-- available under [[cdn-url:2019/os-imt/rsc/art.sh][~rsc/art.sh~]] -- that installs the [[https://github.com/cmatsuoka/figlet][~figlet~]] and
[[https://github.com/busyloop/lolcat][~lolcat~]] softwares on the VM.

#+CAPTION: ~cloud-init~ script available under [[cdn-url:2019/os-imt/rsc/art.sh][~rsc/art.sh~]]
#+NAME: lst:art.sh
#+BEGIN_SRC bash :tangle ./rsc/art.sh
#!/usr/bin/env bash
# Fix DNS resolution
echo "" >> /etc/resolv.conf
echo "nameserver 8.8.8.8" >> /etc/resolv.conf

# Install figlet and lolcat
apt update
apt install -y figlet lolcat
#+END_SRC

#+BEGIN_comment
# solution
#+BEGIN_SRC bash
openstack server create --wait --image debian-9 \
  --flavor m1.mini --network test \
  --key-name admin \
  --user-data ./rsc/art.sh \
  art-vm
#+END_SRC
#+END_comment
# solution

You can follow the correct installation of software with:
: watch openstack console log show --lines=20 art-vm

Could you notice /when/ the VM has finished to boot based on the
~console log~ output?
#+BEGIN_comment
# solution
#+BEGIN_src bash :tangle ./rsc/wordpress-deploy.sh :shebang #!/usr/bin/env bash
CLOUDINIT_END_RX="Cloud-init v\. .\+ finished"
function wait_contextualization {
  local vm="$1"
  local console_log=$(openstack console log show --lines=20 "${vm}")
  while ! echo "${console_log}"|grep -q "${CLOUDINIT_END_RX}"
  do
      echo "Waiting for cloud-init to finish..."
      echo "Current status is:"
      echo "${console_log}"
      sleep 5
      console_log=$(openstack console log show --lines=20 "${vm}")
      # Clear the screen (cuu1 move cursor up by one line, el clear the line)
      for i in {1..22}
      do
          tput cuu1; tput el
      done
  done

  echo "cloud-init finished: "
  echo "${console_log}"|grep "${CLOUDINIT_END_RX}"
}
#+END_src

Then use it as the following.
: wait_contextualization art-vm
#+END_comment
# solution
#+END_do

Then, attach it a floating IP.
#+BEGIN_SRC bash
openstack server add floating ip \
  art-vm \
  $(openstack floating ip create -c floating_ip_address -f value external)
#+END_SRC

Hence, you can jump on the VM and call the ~figlet~ and ~lolcat~
software.
#+BEGIN_example
$ openstack server ssh art-vm \
    --login debian \
    --identity ./admin.pem

The authenticity of host '10.20.20.13 (10.20.20.13)' can't be established.
ECDSA key fingerprint is SHA256:WgAn+/gWYg9MkauihPyQGwC0LJ8sLWM/ySrUzN8cK9w.
Are you sure you want to continue connecting (yes/no)? yes

debian@art-vm:~$ figlet "The Art of Contextualizing a VM" | lolcat
#+END_example

** Run VMs with KVM
Every time you do an ~openstack server create ...~, your request hits,
at some point, the ~nova~ services. It starts by the ~nova-api~ that
processes the REST request. The API, in turns, calls the
~nova-conductor~ that orchestrates the boot: performs some checks,
finds eligible computes and chooses one to transmit the boot order to
its ~nova-compute~. Finally, the ~nova-compute~ asks to the underlying
hypervisor to start the VM.

In your current setup, the hypervisor of your ~nova-compute~ runs
[[https://en.wikipedia.org/wiki/QEMU][QEMU]]. QEMU is a free emulator for hardware virtualization. It supports
a large variety of guest operating systems, but the emulation is a bit
slow. Fortunately, QEMU can be used with [[http://www.linux-kvm.org/][KVM]] to run virtual machines
at near-native speed. KVM (Kernel-based Virtual Machine) is a free
full virtualization solution for Linux that takes advantage of x86
hardware extensions (Intel VT or AMD-V).

To check if your x86 provides hardware virtualization, execute the
following command.
: egrep -c '(vmx|svm)' /proc/cpuinfo
If it outputs a number greater than 0, then proceed with the following
to speed up the VMs execution.

#+BEGIN_do
- Check that the KVM kernel module is loaded, and load it otherwise.
  Seek the [[https://wiki.archlinux.org/index.php/KVM#Kernel_support][archlinux wiki]] for some help.
  #+BEGIN_comment
# solution
  : lsmod|fgrep kvm
  : modprobe -r kvm_intel
  #+END_comment
# solution

- Change the configuration of ~nova-compute~ hypervisor (file
  ~/var/snap/microstack/common/etc/nova/nova.conf.d/hypervisor.conf~)
  to support KVM and reload it.
  #+BEGIN_comment
# solution
  : NOVA_HYPERV_CONF=/var/snap/microstack/common/etc/nova/nova.conf.d/hypervisor.conf
  : sudo sed -i 's|virt_type.\+|virt_type = kvm|' $NOVA_HYPERV_CONF
  : sudo sed -i 's|cpu_mode.\+|cpu_mode = host-passthrough|' $NOVA_HYPERV_CONF
  : sudo snap restart microstack.nova-compute
  #+END_comment
# solution
#+END_do

Finally, create a new VM such as in the [[#sec:cloud-init][previous section]] and
appreciate how fast your VM displays the ~figlet "The Art of
Contextualizing a VM" | lolcat~ command.

* Deploy a WordPress as a Service (as a DevOps)
In the previous sessions, we saw how to boot a VM with OpenStack, and
execute a post-installation script using the ~user-data~ mechanism.
Such mechanism can help us to install software but it is not enough to
deploy a real Cloud application. Cloud applications are composed of
multiple services that collaborate to deliver the application. Each
service is in charge of one aspect of the application. This separation
of concerns brings flexibility. If a single service is overloaded, it
is common to deploy new units of this service to balance the load.

Let's take a simple example: [[https://wordpress.org/][WordPress]]! WordPress is a very popular
content management system (CMS) in use on the Web. People use it to
create websites, blogs or applications. It is open-source, written in
PHP and composed of two elements: a Web server (Apache) and database
(MariaDB). Apache serves the PHP code of WordPress and stores its
information in the database.

Automation is a very important concept for DeVops. Imagine you have
your own datacenter and want to exploit it by renting WordPress
instances to your customers. Each time a client rents an instance, you
have to manually deploy it. Wouldn't it be more convenient to automate
all the operations? 😎

#+BEGIN_do
As the DevOps of {{{co}}} -- {{{c5o}}} -- your job is to automatize
the deployment of WordPress on your OpenStack. To do so, you have to
make a bash script that:

1. Starts ~wordpress-db~: a VM that contains the MariaDB database for
   WordPress.
2. Waits until its final deployment (the database is running)
3. Starts ~wordpress-app~: a VM that contains a web server and serves
   the Wordpress CMS.
4. Finally, connects to the WordPress website and initializes a new
   WordPress project named ~os-oph~.

The ~rsc~ directory provides bash scripts to deploy the MariaDB
database and web server of WordPress (also in [[*Appendix][Appendix]]). Review it
before going further (spot the *TODO*).

Also, remind to [[#sec:debian9-ftw][clean your environment]].

#+BEGIN_comment
# solution
Find the solution in the [[file:rsc/wordpress-deploy.sh][~rsc/wordpress-deploy.sh~]] script.

First thing first, enable HTTP connections.
#+BEGIN_SRC bash
openstack security group rule create $SECGROUP_ID \
  --proto tcp --remote-ip 0.0.0.0/0 \
  --dst-port 80
#+END_SRC

Then start a VM with the ~wordpress-db~ name, ~debian-9~ image,
~m1.mini~ flavor, ~test~ network and ~admin~ key-pair. Also,
contextualize your VM with the [[file:rsc/install-mariadb.sh][~rsc/install-mariadb.sh~]] script thanks
to the ~--user-data ./rsc/install-mariadb.sh~ option.

#+BEGIN_SRC bash :tangle ./rsc/wordpress-deploy.sh
openstack server create --wait --image debian-9 \
  --flavor m1.mini --network test \
  --key-name admin \
  --user-data ./rsc/install-mariadb.sh \
  wordpress-db

wait_contextualization wordpress-db
#+END_SRC

Next, start a VM with ~wordpress-app~ name, ~debian-9~ image,
~m1.mini~ flavor, ~test~ network and ~admin~ key-pair. Also,
contextualize your VM with the [[file:rsc/install-wp.sh][~rsc/install-wp.sh~]] script thanks to
the ~--user-data ./rsc/install-wp.sh~ option. Note that you need to
provide the IP address of the ~wordpress-db~ to this script before
running it.

Set the script with IP address of ~wordpress-db~ # and floating ip
#+BEGIN_SRC bash :tangle ./rsc/wordpress-deploy.sh
sed -i '13s|.*|DB_HOST="'$(openstack server show wordpress-db -c addresses -f value | sed -Er "s/test=//g")'"|' ./rsc/install-wp.sh
#+END_SRC

Then, create ~wordpress-app~.
#+BEGIN_SRC bash :tangle ./rsc/wordpress-deploy.sh :shebang #!/usr/bin/env bash
openstack server create --wait --image debian-9 \
  --flavor m1.mini --network test \
  --key-name admin \
  --user-data ./rsc/install-wp.sh \
  wordpress-app

wait_contextualization wordpress-app
#+END_SRC

Get a floating ip for the VM.
#+BEGIN_SRC bash :tangle ./rsc/wordpress-deploy.sh
WP_APP_FIP=$(openstack floating ip create -c floating_ip_address -f value external)
#+END_SRC

Attach the ~WP_APP_FIP~ floating ip to that VM.
#+BEGIN_SRC bash :tangle ./rsc/wordpress-deploy.sh
openstack server add floating ip wordpress-app "${WP_APP_FIP}"
#+END_SRC

Setup redirection to access your floating ip on port 80.
: sudo iptables -t nat -A PREROUTING -p tcp --dport 8081 -j DNAT --to "${WP_APP_FIP}:80"

Finally, you can reach WordPress on [[http://<ip-of-your-lab>:8080/wp]].

#+BEGIN_note
Optionally, you can do it with an SSH tunnel to access ~10.20.20.*~
from your own machine.
: ssh -NL 8080:<floating-ip>:80 -l root <ip-of-your-lab-machine>

Then, reach WordPress on [[http://localhost:8080/wp]].
#+END_note
#+END_comment
# solution
#+END_do

* COMMENT Automatize the deployment with Heat
[[https://wiki.openstack.org/wiki/Heat][Heat]] is the OpenStack orchestrator: it eats templates (called HOT for
Heat Orchestration Template - which are files written in YAML)
describing the OpenStack infrastructure you want to deploy (e.g. VMs,
networks, storages) as well as software configurations. Then the Heat
engine is in charge of sending the appropriate requests to OpenStack
to deploy the system described in your template (deployments are
called ~stacks~ in Heat). In the following subsections, we are going
to manipulate Heat to understand how to deploy applications on
OpenStack. The following examples are extracted from the heat
templates you can find under the ~rsc/heat-templates/~ directory.

** Preamble
In this last part, the teacher has setup an OpenStack in a DataCenter
(here, on top of Grid'5000) and created member account and project for
each of you (not admin). As a preamble, you should:
- Connect to the Grid'5000 VPN (see §[[#sec:assign-lab]]).
- Go on the [[os-cloud][horizon dashbord]] of teacher's OpenStack and download the
  "OpenStack RC File V3" (see §[[#sec:os-cli]]) on your own machine.
  + user name: your Grid'5000 login
  + password: ~os-imt~
- [[https://github.com/openstack/python-openstackclient/tree/stable/rocky#getting-started][Install the OpenStack CLI]] and [[https://github.com/openstack/python-heatclient/tree/stable/rocky][Heat CLI]] on your own machine.
  #+begin_src bash
  sudo apt upgrade && sudo apt install pip -y
  sudo pip install --upgrade pip
  sudo pip install python-openstackclient python-heatclient
  pip show python-openstackclient # note the location path. If the
  # location path is in your $HOME, then
  echo export 'PATH=$HOME/.local/bin:$PATH' >> $HOME/.bashrc
  #+end_src
  # : alias openstack="pipenv run openstack"
- Source the "OpenStack RC File V3".
- Reimport (or maybe recreate) your admin SSH key (see
  §[[#sec:enc-trust]]).
- Download the last version of the Lab resources (see §[[#sec:rscs-lab]]).

Resource names change a bit from previously. Do not hesitate to run
some commands such as the following to know about new names.
- ~openstack network list~
- ~openstack image list~
- ~openstack flavor list~
- ...

** Boot a VM
The simplest HOT template your can declare describes how to boot a VM:

#+BEGIN_SRC yaml :tangle rsc/heat-templates/1_boot_vm.yaml
# The following heat template version tag is mandatory:
heat_template_version: 2017-09-01

# Here we define a simple decription of the template (optional):
description: >
    Simply boot a VM!

# Here we declare the resources to deploy.
# Resources are defined by a name and a type which described many properties:
resources:
    # Name of my resource:
    my_vm:
        # Its type, here we want to define an OpenStack Nova server:
        type: "OS::Nova::Server"
        properties:
            name: hello_world      # Name of the VM
            image: debian-9        # Its image of the VM (must be available in Glance)
            flavor: m1.mini        # Its flavor (must exist in Nova)
            key_name: admin        # Name of the SSH Key (must exist in Nova)
            networks:              # List of networks to connect to
              - {network: private}
#+END_SRC

As depicted in this example, the different OpenStack resources can be
declared using types. OpenStack resource types are listed in the
[[https://docs.openstack.org/heat/pike/template_guide/openstack.html][documentation]], browsing this page, you can see that resources exist
for most OpenStack services (e.g. Nova, Neutron, Glance, Cinder,
Heat). Here, we declare a new resource called ~my_vm~ which is defined
by the type ~OS::Nova::Server~ to declare a new virtual machine. A
type defines different properties (some are mandatory, some are
optional, see the documentation for more details). The
~OS::Nova::Server~ properties should be familiar to you since it is
the classical properties Nova requires to boot a VM (i.e. a name, an
image, a flavor, a key name). Once you have written this template in a
file, you can now deploy the stack as following:

#+BEGIN_SRC bash
openstack stack create -t ./rsc/heat-templates/1_boot_vm.yaml hw1
openstack stack list
openstack stack show hw1
watch openstack server list
openstack stack delete hw1
#+END_SRC

This simple template is enough to run a virtual machine. However it is
very static. In the next subsection, we are going to manipulate
parameters to add flexibility.

** Need more flexibility: let's add parameters!

Templates can be more flexible with parameters. To that end you can:
- Declare a set of parameters to provide to your template.
- Use the intrinsic function ~get_param~ to map those parameters in
  your resource declarations.
Here's an example:

#+begin_src yaml :tangle rsc/heat-templates/2_boot_vm_with_params.yaml
heat_template_version: 2017-09-01

description: >
    Simply boot a VM with params!

# Here we define parameters
# Parameters have a name, and a list of properties:
parameters:
    param_vm_name:
        type: string                               # the type of the parameter (required)
        description: Name of the server            # an optional description
    param_image:
        type: string
        description: Image to use for servers
        default: debian-9                          # an optional default value
    param_flavor:
        type: string
        description: Flavor to use for servers
        default: m1.small
    param_key:
        type: string
        description: Key name to use for servers
        default: admin

# Here we use intrinsic functions to get the parameters:
resources:
    my_vm:
        type: "OS::Nova::Server"
        properties:
            name: { get_param: param_vm_name }
            image: { get_param: param_image }
            flavor: { get_param: param_flavor }
            key_name: { get_param: param_key }
            networks:
              - {network: private}
#+end_src


In this example, we defined two parameters. While the first one
related to the VM flavor has a default value (i.e. ~m1.small~), the
second one, corresponding to the name of the key pair to use, must be
provided. To deploy this stack, run the following command:

#+BEGIN_src bash
openstack stack create -t ./rsc/heat-templates/2_boot_vm_with_params.yaml \
  --parameter param_vm_name=hello_params \
  --parameter param_flavor=m1.small \
  hw2
openstack server list
openstack stack delete hw2
#+END_src

This command deploys our VM by overriding the default flavor value
~m1.mini~ by ~m1.small~. This can be checked by typing: ~openstack
server list~. The parameter ~param_vm_name~ is required and no default
value is provided. As such, if you try to create a stack without
providing this parameter, you end with the following error:

#+BEGIN_SRC bash
openstack stack create -t ./rsc/heat-templates/2_boot_vm_with_params.yaml \
    --parameter param_flavor=m1.medium \
    hw2_error
ERROR: The Parameter (param_vm_name) was not provided.
#+END_SRC

Parameters are the inputs of our templates. In the next subsection, we
are going to see how templates can declare outputs, so that our stacks
can return a set of attributes (e.g., the IP address of a deployed
VM).

** Need our deployment to return values: let's use outputs!
Templates can declare a set of attributes to return. For instance, you
might need to know the IP address of a resource at run-time. To that
end, you can declare attributes in a new section called ~outputs~:

#+begin_src yaml :tangle rsc/heat-templates/3_boot_vm_with_output.yaml
heat_template_version: 2017-09-01

description: >
    Boot a VM and return its IP address!

resources:
    my_vm:
        type: "OS::Nova::Server"
        properties:
            name: hello_outputs
            image: debian-9
            flavor: m1.mini
            key_name: admin
            networks:
              - {network: private}

# We set here outputs (stack returned attributes).
# Outputs are defined by a name, and a set of properties:
outputs:
    HOSTIP:
        description: IP address of the created instance    # The name is optional
        value: { get_attr: [my_vm, first_address] }        # The value of this attribute
#+end_src

We declared here an output attribute called ~HOSTIP~ which stores the
IP address of the VM resource. We used here another intrinsic function
which is used to get the IP address from our VM: ~get_attr~. Output
attributes can be exploited in two ways: it can be displayed from the
CLI, or it can be fetched by other stack templates (we will see this
last case latter):

#+BEGIN_src bash
openstack stack create -t ./rsc/heat-templates/3_boot_vm_with_output.yaml hw3
openstack stack output list hw3
openstack stack output show hw3 HOSTIP
openstack stack delete hw3
#+END_src

** Integrate ~cloud-init~ in Heat
It is possible to declare a post-installation script in the template
with the user-data property:

#+begin_src yaml :tangle rsc/heat-templates/4_boot_vm_with_user-data.yaml
heat_template_version: 2017-09-01

description: >
    Boot a VM with a post-installation script!

resources:
    my_vm:
        type: "OS::Nova::Server"
        properties:
            name: hello_cloud_init
            image: debian-9
            flavor: m1.mini
            key_name: admin
            networks:
              - {network: private}
            # We set here the user-data:
            user_data: |
                #!/usr/bin/env bash
                apt-get update
                apt-get install -y fortune fortunes cowsay lolcat
                echo -e "#!/bin/bash\n\nfortune | cowsay -n | lolcat\necho" \
                         > /etc/profile.d/cowsay.sh
#+end_src

#+BEGIN_src bash
openstack stack create -t ./rsc/heat-templates/4_boot_vm_with_user-data.yaml hw4
#+END_src

Associating a floating IP is a bit tricky with Heat, so let's do it
manually for now. Then, wait for ~cloud-init~ to finish and finally,
SSH on the VM.

#+begin_src bash
openstack server add floating ip hello_cloud_init \
  $(openstack floating ip create -c floating_ip_address -f value public)
wait_contextualization hello_cloud_init
openstack server ssh --login debian --identity ./admin.pem hello_cloud_init
openstack stack delete hw4
#+end_src

** Dynamic configuration with ~cloud-init~ and parameters
Let's mix the capabilities we learned from the parameter and
cloud-init templates to write a template with a flexible
post-installation script. With Heat, it is possible to provide a
parameter to your user-data at run-time by using a new function:
~str_replace~!

#+begin_src yaml :tangle rsc/heat-templates/5_boot_vm_with_user-data2.yaml
heat_template_version: 2017-09-01

description: >
    Boot a VM by installing a set of packages given as parameters!

parameters:
    PackageName:
        label: List of packages to install
        type: string

resources:
    my_vm:
        type: "OS::Nova::Server"
        properties:
            name: hello_cloud_init_params
            image: debian-9
            flavor: m1.mini
            key_name: admin
            networks:
              - {network: private}
            user_data:
                # This intrinsic function can replace strings in user-data:
                str_replace:
                    # We define here the parameters for our script
                    params:
                        ${PACKAGE_NAME}: { get_param: PackageName }
                    # We define here the script
                    template: |
                        #!/usr/bin/env bash
                        apt-get update
                        apt-get install -y ${PACKAGE_NAME}
#+end_src

We used here the new intrinsic function ~str_replace~ to replace
strings in our user-data. In this example, the parameter should be a
string containing a set of packages to install in the VM. You can
deploy the stack as follow:

#+BEGIN_SRC bash
openstack stack create \
    -t ./rsc/heat-templates/5_boot_vm_with_user-data2.yaml \
    --parameter PackageName="vim cowsay fortune fortunes lolcat" \
   hw5
#+END_SRC

This mechanism is crucial to dynamically configure our services during
the deployment. For instance, ~service_A~ might require an IP address
in its configuration file to access ~service_B~, which runs on another
VM. This IP address is only known at run-time, so it must be
represented by a variable managed in Heat templates. In the next
subsections, we are going to study how to declare such variable, so
that Heat resources can exchange information.

** Data dependency between resources
:PROPERTIES:
:CUSTOM_ID: sec:data-deps-rscs
:END:
Let's declare a template with two VMs: ~provider~ and ~user~. The idea is to
configure user's static lookup table for hostnames (more information can be
found by typing: ~man hosts~), so that user can target provider from its
hostname rather than from its IP address. To that end, we will use the user-data
mechanism to edit the ~/etc/hosts~ file on user, and map the IP address of
provider with its hostname:

#+begin_src yaml :tangle rsc/heat-templates/6_boot_vms_with_exchange.yaml
heat_template_version: 2017-09-01

description: >
    Boot two VMs and ease the access from user to provider!

resources:
    my_provider_vm:
        type: "OS::Nova::Server"
        properties:
            name: provider
            image: debian-9
            flavor: m1.mini
            key_name: admin
            networks:
              - {network: private}
    my_user_vm:
        type: "OS::Nova::Server"
        properties:
            name: user
            image: debian-9
            flavor: m1.mini
            key_name: admin
            networks:
              - {network: private}
            user_data:
                str_replace:
                    params:
                        ${IP_ADDRESS}: { get_attr: [my_provider_vm, first_address] }
                    template: |
                        #!/usr/bin/env bash
                        # With the following line, provider is reachable from its hostname
                        echo "${IP_ADDRESS} provider" >> /etc/hosts
#+end_src

In this example, ~user~ requires the IP address of ~provider~ to boot.
The Heat engine is in charge of managing dependencies between
resources. Take a look during the deployment, and check that
~provider~ is deployed prior ~user~:

#+BEGIN_src bash
openstack stack create -t ./rsc/heat-templates/6_boot_vms_with_exchange.yaml hw6 \
  && watch openstack server list
openstack server add floating ip user \
  $(openstack floating ip create -c floating_ip_address -f value public)
openstack server ssh --login debian --identity ./admin.pem --address-type public user
debian@user:~$ ping provider
debian@user:~$ exit
openstack stack delete hw6
#+END_SRC

** Nested templates
Heat is able to compose templates to keep human-readable files, using
nested templates. For instance, we can use a first template that
describes a virtual machine, and a second template which deploys
multiple VMs by referencing the first one. Rather than create the
first template, we can re-use
[[cdn-url:2019/os-imt/rsc/heat-templates/2_boot_wm_with_params.yaml][~rsc/heat-templates/2_boot_vm_with_params.yaml~]]:

#+begin_src yaml :tangle rsc/heat-templates/7_nested_template.yaml
heat_template_version: 2017-09-01

description: >
    Boot two different VMs by exploiting nested templates!

resources:
    my_provider_vm:
        # Template can be provided as resource type (relatively to
        # that template)
        type: ./2_boot_vm_with_params.yaml
        # The related properties are given as template's parameters:
        properties:
            param_vm_name: provider
            param_flavor: m1.medium
    my_user_vm:
        type: ./2_boot_vm_with_params.yaml
        properties:
            param_vm_name: user
#+end_src

To compose template, a new resource can be defined by specifying its
type as the target of the desired template. A set of properties can be
provided to the nested template and will be interpreted as parameters.

#+BEGIN_src bash
openstack stack create -t ./rsc/heat-templates/7_nested_template.yaml hw7 \
  && watch openstack server list
openstack stack delete hw7
#+END_SRC

Nested templates are very convenient to keep your code clean and
re-use templates. We are now reaching the last subsection, where we
are going to extend nested templates with data dependency.

** Nested templates with data dependency
Let's describe the same deployment as in [[#sec:data-deps-rscs][Data dependency between
resources]] by using nested templates. For that we need a new template:

#+begin_src yaml :tangle rsc/heat-templates/8_nested_template_boot_vm.yaml
heat_template_version: 2017-09-01

description: >
    Boot a VM, ease access to a remote host and return its IP address!

parameters:
    param_vm_name:
        type: string
        description: Name of the server
    param_image:
        type: string
        description: Image to use for servers
        default: debian-9
    param_flavor:
        type: string
        description: Flavor to use for servers
        default: m1.small
    param_key:
        type: string
        description: Key name to use for servers
        default: admin
    param_remote_hostname:
        type: string
        description: Host name of the remote host
        default: provider
    param_remote_ip:
        type: string
        description: IP address of the remote host

resources:
    my_vm:
        type: "OS::Nova::Server"
        properties:
            name: { get_param: param_vm_name }
            image: { get_param: param_image }
            flavor: { get_param: param_flavor }
            key_name: { get_param: param_key }
            networks:
              - {network: private}
            user_data:
                str_replace:
                    params:
                        ${HOSTNAME}: { get_param: param_remote_hostname }
                        ${IP_ADDRESS}: { get_param: param_remote_ip }
                    template: |
                        #!/bin/bash
                        # With the following line, the remote host is reachable from its hostname
                        echo "${IP_ADDRESS} ${HOSTNAME}" >> /etc/hosts

outputs:
    HOSTNAME:
        description: IP address of the created instance
        value: { get_attr: [my_vm, hostname] }
    HOSTIP:
        description: IP address of the created instance
        value: { get_attr: [my_vm, first_address] }
#+end_src

We can now declare the main template. While it defines three VMs, this
template is easy to read since it points to the template created
previously, and ~3_boot_vm_with_output.yaml~:

#+begin_src yaml :tangle rsc/heat-templates/8_nested_template_exchange.yaml
heat_template_version: 2017-09-01

description: >
    Boot three VMs and ease the access to provider using nested
    templates!

resources:
    my_provider_vm:
        type: ./3_boot_vm_with_output.yaml
        properties:
            param_vm_name: provider

    my_user_vm1:
        type: ./8_nested_template_boot_vm.yaml
        properties:
            param_vm_name: user1
            param_remote_ip: { get_attr: [my_provider_vm, HOSTIP] }

    my_user_vm2:
        type: ./8_nested_template_boot_vm.yaml
        properties:
            param_vm_name: user2
            param_remote_ip: { get_attr: [my_provider_vm, HOSTIP] }
#+end_src

** TODO COMMENT Other type of resources
Add an new template that require a floating ip as in the WordPress
solution ~web-vm.yml~.

* COMMENT Deploy a WordPress as a Service (as a Heat DevOps)
As a DevOps at {{{co}}} -- {{{c5o}}} -- you are now in charge of the
automation process of deploying WordPress instances for clients.
Congratulation! To that end, you have to use what you learned from the
previous section to design a template that describes a WordPress
application using Heat. We are going to deploy WordPress inside two
VMs: the first one holds the web server, the second one runs the
database:

- VM1: Apache + PHP + WordPress code
- VM2: MariaDB

#+BEGIN_do
Create three HOT files:

- ~sql-vm.yml~: containing the description of the VM running MariaDB;
- ~web-vm.yml~: containing the description of the VM running the Web server;
- ~wp-app.yml~: containing the description of the WordPress application
  (~sql-vm.yml~ + ~web-vm.yml~ as nested templates).

Once it is deployed, you should be able to reach the wordpress service by
going on [[http://<web-server-ip-address>/wp]].

#+BEGIN_comment
# solution
Find the solution in the [[cdn-url:2019/os-imt/rsc/heat-templates/wordpress/][~rsc/heat-templates/wordpress/~]] directory.
#+END_comment
# solution
#+END_do

* Appendix
** Install MariaDB on Debian 9
#+BEGIN_src bash :tangle ./rsc/install-mariadb.sh
#!/usr/bin/env bash
#
# Install and configure MariaDB for Debian 9.

# Fix DNS resolution
echo "" >> /etc/resolv.conf
echo "nameserver 8.8.8.8" >> /etc/resolv.conf

# Parameters
DB_ROOTPASSWORD=root
DB_NAME=wordpress    # Wordpress DB name
DB_USER=silr         # Wordpress DB user
DB_PASSWORD=silr     # Wordpress DB pass

# Install MariaDB
apt update -q
apt install -q -y mariadb-server mariadb-client

# Next line stops mysql install from popping up request for root password
export DEBIAN_FRONTEND=noninteractive
sed -i 's/127.0.0.1/0.0.0.0/' /etc/mysql/mariadb.conf.d/50-server.cnf
systemctl restart mysql

# Setup MySQL root password and create a user and add remote privs to app subnet
mysqladmin -u root password ${DB_ROOTPASSWORD}

# Create the wordpress database
cat << EOSQL | mysql -u root --password=${DB_ROOTPASSWORD}
FLUSH PRIVILEGES;
CREATE USER '${DB_USER}'@'localhost';
CREATE DATABASE ${DB_NAME};
SET PASSWORD FOR '${DB_USER}'@'localhost'=PASSWORD("${DB_PASSWORD}");
GRANT ALL PRIVILEGES ON ${DB_NAME}.* TO '${DB_USER}'@'localhost' IDENTIFIED BY '${DB_PASSWORD}';
CREATE USER '${DB_USER}'@'%';
SET PASSWORD FOR '${DB_USER}'@'%'=PASSWORD("${DB_PASSWORD}");
GRANT ALL PRIVILEGES ON ${DB_NAME}.* TO '${DB_USER}'@'%' IDENTIFIED BY '${DB_PASSWORD}';
EOSQL
#+END_src

** Install Wordpress application on Debian 9
#+BEGIN_src bash :tangle ./rsc/install-wp.sh
#!/usr/bin/env bash
#
# Install and configure Apache to serve Wordpress for Debian 9.

# Fix DNS resolution
echo "" >> /etc/resolv.conf
echo "nameserver 8.8.8.8" >> /etc/resolv.conf

# Parameters
DB_NAME=wordpress
DB_USER=silr
DB_PASSWORD=silr
DB_HOST=<TODO>

apt-get update -y
apt-get upgrade -y
apt-get install -q -y --force-yes wordpress apache2 curl

cat << EOF > /etc/apache2/sites-available/wp.conf
Alias /wp/wp-content /var/lib/wordpress/wp-content
Alias /wp /usr/share/wordpress
<Directory /usr/share/wordpress>
    Options FollowSymLinks
    AllowOverride Limit Options FileInfo
    DirectoryIndex index.php
    Require all granted
</Directory>
<Directory /var/lib/wordpress/wp-content>
    Options FollowSymLinks
    Require all granted
</Directory>
EOF

a2ensite wp
service apache2 reload

cat << EOF > /etc/wordpress/config-default.php
<?php
define('DB_NAME', '${DB_NAME}');
define('DB_USER', '${DB_USER}');
define('DB_PASSWORD', '${DB_PASSWORD}');
define('DB_HOST', '${DB_HOST}');
define('WP_CONTENT_DIR', '/var/lib/wordpress/wp-content');
?>
EOF
#+END_src
# This is not needed anymore.
# define('WP_SITEURL', 'http://' . $_SERVER['HTTP_HOST'] . '/wp');
